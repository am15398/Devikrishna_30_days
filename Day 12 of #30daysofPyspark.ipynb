{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "982089b9-1926-4fce-bdfa-63784beee338",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04405e1f-295c-4ece-9e5f-64c47d858a8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n|user_id|          timestamp|\n+-------+-------------------+\n|  user1|2023-08-26 10:10:00|\n|  user1|2023-08-26 10:10:25|\n|  user2|2023-08-26 12:00:00|\n|  user2|2023-08-26 12:10:00|\n|  user1|2023-08-27 14:30:00|\n|  user1|2023-08-28 16:00:00|\n|  user2|2023-08-27 16:30:00|\n|  user1|2023-08-29 18:00:00|\n+-------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"TopConsecutiveUsers\").getOrCreate()\n",
    "\n",
    "# Define the schema for your data\n",
    "schema = [\"user_id\", \"timestamp\"]\n",
    "data = [\n",
    "    (\"user1\", \"2023-08-26 10:10:00\"),\n",
    "    (\"user1\", \"2023-08-26 10:10:25\"),\n",
    "    (\"user2\", \"2023-08-26 12:00:00\"),\n",
    "    (\"user2\", \"2023-08-26 12:10:00\"),\n",
    "    (\"user1\", \"2023-08-27 14:30:00\"),\n",
    "    (\"user1\", \"2023-08-28 16:00:00\"),\n",
    "    (\"user2\", \"2023-08-27 16:30:00\"),\n",
    "    (\"user1\", \"2023-08-29 18:00:00\"),\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ee7b99f-eccd-4d52-b800-f8de56854ba4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+\n|user_id|          timestamp|      date|\n+-------+-------------------+----------+\n|  user1|2023-08-26 10:10:00|2023-08-26|\n|  user1|2023-08-26 10:10:25|2023-08-26|\n|  user2|2023-08-26 12:00:00|2023-08-26|\n|  user2|2023-08-26 12:10:00|2023-08-26|\n|  user1|2023-08-27 14:30:00|2023-08-27|\n|  user1|2023-08-28 16:00:00|2023-08-28|\n|  user2|2023-08-27 16:30:00|2023-08-27|\n|  user1|2023-08-29 18:00:00|2023-08-29|\n+-------+-------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Convert the timestamp column to a date type\n",
    "df = df.withColumn(\"date\", to_date(\"timestamp\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "581ae378-4151-40e7-900c-834c7058b523",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+----------+---------+\n|user_id|          timestamp|      date|    lag_dt|date_diff|\n+-------+-------------------+----------+----------+---------+\n|  user1|2023-08-26 10:10:00|2023-08-26|      null|     null|\n|  user1|2023-08-26 10:10:25|2023-08-26|2023-08-26|        0|\n|  user1|2023-08-27 14:30:00|2023-08-27|2023-08-26|        1|\n|  user1|2023-08-28 16:00:00|2023-08-28|2023-08-27|        1|\n|  user1|2023-08-29 18:00:00|2023-08-29|2023-08-28|        1|\n|  user2|2023-08-26 12:00:00|2023-08-26|      null|     null|\n|  user2|2023-08-26 12:10:00|2023-08-26|2023-08-26|        0|\n|  user2|2023-08-27 16:30:00|2023-08-27|2023-08-26|        1|\n+-------+-------------------+----------+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Define a window specification to partition by user and order by date\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(\"date\")\n",
    "df=df.withColumn(\"lag_dt\",F.lag(\"date\").over(window_spec))\n",
    "# Calculate the difference between consecutive dates\n",
    "df = df.withColumn(\"date_diff\", F.datediff(\"date\",\"lag_dt\" ))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb111211-ab37-439d-b73c-b49ff8a2fe0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+----------+---------+--------------+\n|user_id|          timestamp|      date|    lag_dt|date_diff|is_consecutive|\n+-------+-------------------+----------+----------+---------+--------------+\n|  user1|2023-08-26 10:10:00|2023-08-26|      null|     null|             0|\n|  user1|2023-08-26 10:10:25|2023-08-26|2023-08-26|        0|             0|\n|  user1|2023-08-27 14:30:00|2023-08-27|2023-08-26|        1|             1|\n|  user1|2023-08-28 16:00:00|2023-08-28|2023-08-27|        1|             1|\n|  user1|2023-08-29 18:00:00|2023-08-29|2023-08-28|        1|             1|\n|  user2|2023-08-26 12:00:00|2023-08-26|      null|     null|             0|\n|  user2|2023-08-26 12:10:00|2023-08-26|2023-08-26|        0|             0|\n|  user2|2023-08-27 16:30:00|2023-08-27|2023-08-26|        1|             1|\n+-------+-------------------+----------+----------+---------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Identify consecutive login sequences\n",
    "df = df.withColumn(\"is_consecutive\", F.when(F.col(\"date_diff\") == 1, 1).otherwise(0))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53ba4094-cfa6-4ff0-8cdc-22a14b1c923f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+----------+---------+--------------+---------------+\n|user_id|          timestamp|      date|    lag_dt|date_diff|is_consecutive|consecutive_sum|\n+-------+-------------------+----------+----------+---------+--------------+---------------+\n|  user1|2023-08-26 10:10:00|2023-08-26|      null|     null|             0|              0|\n|  user1|2023-08-26 10:10:25|2023-08-26|2023-08-26|        0|             0|              0|\n|  user1|2023-08-27 14:30:00|2023-08-27|2023-08-26|        1|             1|              1|\n|  user1|2023-08-28 16:00:00|2023-08-28|2023-08-27|        1|             1|              2|\n|  user1|2023-08-29 18:00:00|2023-08-29|2023-08-28|        1|             1|              3|\n|  user2|2023-08-26 12:00:00|2023-08-26|      null|     null|             0|              0|\n|  user2|2023-08-26 12:10:00|2023-08-26|2023-08-26|        0|             0|              0|\n|  user2|2023-08-27 16:30:00|2023-08-27|2023-08-26|        1|             1|              1|\n+-------+-------------------+----------+----------+---------+--------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Calculate the cumulative sum of consecutive logins\n",
    "df = df.withColumn(\"consecutive_sum\", F.sum(\"is_consecutive\").over(window_spec))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73956478-f752-412c-80f8-7aa72f0ae56b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n|user_id|max_consecutive_days|\n+-------+--------------------+\n|  user1|                   3|\n|  user2|                   1|\n+-------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find the maximum consecutive logins for each user\n",
    "max_consecutive_df = df.groupBy(\"user_id\").agg(F.max(\"consecutive_sum\").alias(\"max_consecutive_days\"))\n",
    "max_consecutive_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0134ef3f-ee5c-4181-a77c-74ae48b7dc45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n|user_id|max_consecutive_days|\n+-------+--------------------+\n|  user1|                   3|\n+-------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Find the top 1 users with the most consecutive logins\n",
    "top_1_users = max_consecutive_df.orderBy(F.desc(\"max_consecutive_days\")).limit(1)\n",
    "\n",
    "top_1_users.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day 12 of #30daysofPyspark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
